{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30732,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "script",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DetlefShrimps/goodeye/blob/main/g00d3y3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybaseball optuna dask[distributed] transformers"
      ],
      "metadata": {
        "_uuid": "daa47728-cd20-4730-a046-d8dd91978be0",
        "_cell_guid": "5339bc1f-efa9-4263-a081-de07cbe69f5f",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-07-02T11:43:20.121906Z",
          "iopub.execute_input": "2024-07-02T11:43:20.122925Z",
          "iopub.status.idle": "2024-07-02T11:43:21.131742Z",
          "shell.execute_reply.started": "2024-07-02T11:43:20.122886Z",
          "shell.execute_reply": "2024-07-02T11:43:21.130543Z"
        },
        "trusted": true,
        "id": "SpiyMXAXQvR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97342884-5012-49a3-81a1-4f0526045162"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybaseball\n",
            "  Downloading pybaseball-2.2.7-py3-none-any.whl (426 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.1/426.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dask[distributed] in /usr/local/lib/python3.10/dist-packages (2023.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from pybaseball) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pybaseball) (2.0.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pybaseball) (4.12.3)\n",
            "Requirement already satisfied: requests>=2.18.1 in /usr/local/lib/python3.10/dist-packages (from pybaseball) (2.31.0)\n",
            "Requirement already satisfied: lxml>=4.2.1 in /usr/local/lib/python3.10/dist-packages (from pybaseball) (4.9.4)\n",
            "Requirement already satisfied: pyarrow>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from pybaseball) (14.0.2)\n",
            "Collecting pygithub>=1.51 (from pybaseball)\n",
            "  Downloading PyGithub-2.3.0-py3-none-any.whl (354 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.4/354.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pybaseball) (1.11.4)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pybaseball) (3.7.1)\n",
            "Requirement already satisfied: tqdm>=4.50.0 in /usr/local/lib/python3.10/dist-packages (from pybaseball) (4.66.4)\n",
            "Requirement already satisfied: attrs>=20.3.0 in /usr/local/lib/python3.10/dist-packages (from pybaseball) (23.2.0)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.31)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask[distributed]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask[distributed]) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[distributed]) (2023.6.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask[distributed]) (1.4.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[distributed]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[distributed]) (8.0.0)\n",
            "Requirement already satisfied: distributed==2023.8.1 in /usr/local/lib/python3.10/dist-packages (from dask[distributed]) (2023.8.1)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[distributed]) (3.1.4)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[distributed]) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[distributed]) (1.0.8)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[distributed]) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[distributed]) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[distributed]) (3.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[distributed]) (6.3.3)\n",
            "Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[distributed]) (2.0.7)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2023.8.1->dask[distributed]) (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.0->pybaseball) (2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[distributed]) (3.19.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pybaseball) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pybaseball) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pybaseball) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pybaseball) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pybaseball) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pybaseball) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pybaseball) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.3->pybaseball) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.3->pybaseball) (2024.1)\n",
            "Collecting pynacl>=1.4.0 (from pygithub>=1.51->pybaseball)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjwt[crypto]>=2.4.0 (from pygithub>=1.51->pybaseball)\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Collecting Deprecated (from pygithub>=1.51->pybaseball)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.1->pybaseball) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.1->pybaseball) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.1->pybaseball) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed==2023.8.1->dask[distributed]) (2.1.5)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->pygithub>=1.51->pybaseball) (42.0.8)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->pygithub>=1.51->pybaseball) (1.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pybaseball) (1.16.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->pygithub>=1.51->pybaseball) (1.14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->pygithub>=1.51->pybaseball) (2.22)\n",
            "Installing collected packages: pyjwt, Mako, Deprecated, colorlog, pynacl, alembic, optuna, pygithub, pybaseball\n",
            "  Attempting uninstall: pyjwt\n",
            "    Found existing installation: PyJWT 2.3.0\n",
            "    Uninstalling PyJWT-2.3.0:\n",
            "      Successfully uninstalled PyJWT-2.3.0\n",
            "Successfully installed Deprecated-1.2.14 Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-3.6.1 pybaseball-2.2.7 pygithub-2.3.0 pyjwt-2.8.0 pynacl-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "from dask.distributed import Client\n",
        "from dask import delayed\n",
        "from pybaseball import statcast, batting_stats\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import optuna\n",
        "import joblib\n",
        "import os\n",
        "import logging\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from transformers import BertModel, BertConfig, BertTokenizer, AdamW\n",
        "from tqdm import tqdm\n",
        "import unittest\n",
        "import time\n",
        "from pybaseball import cache\n",
        "\n",
        "cache.enable()\n",
        "\n",
        "def main():\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "    logger = logging.getLogger()\n",
        "\n",
        "    # Initialize Dask client\n",
        "    client = Client(n_workers=4, threads_per_worker=2, memory_limit='16GB')\n",
        "\n",
        "    # Data Collection\n",
        "    logger.info(\"Collecting data...\")\n",
        "    pitch_data = statcast(start_dt='2008-03-25', end_dt='2024-07-01')\n",
        "    season_stats = batting_stats(2008, 2024)\n",
        "    logger.info(f\"Data collection completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # Data Storage\n",
        "    storage_start_time = time.time()\n",
        "    logger.info(\"Storing data in SQLite database...\")\n",
        "    conn = sqlite3.connect('mlb_data.db')\n",
        "    pitch_data.to_sql('pitch_data', conn, if_exists='replace', index=False)\n",
        "    season_stats.to_sql('season_stats', conn, if_exists='replace', index=False)\n",
        "    logger.info(f\"Data storage completed in {time.time() - storage_start_time:.2f} seconds\")\n",
        "\n",
        "    # Data Preparation\n",
        "    preparation_start_time = time.time()\n",
        "    logger.info(\"Preparing data...\")\n",
        "    pitch_df = dd.read_sql_table('pitch_data', 'sqlite:///mlb_data.db', index_col='index')\n",
        "    season_df = dd.read_sql_table('season_stats', 'sqlite:///mlb_data.db', index_col='index')\n",
        "    logger.info(f\"Data preparation completed in {time.time() - preparation_start_time:.2f} seconds\")\n",
        "\n",
        "    # Feature Engineering: Create gamestate delta, supplement inputs, etc.\n",
        "    def create_gamestate_delta(df):\n",
        "        df['gamestate_delta'] = df['balls'] - df['strikes']\n",
        "        return df\n",
        "\n",
        "    pitch_df = pitch_df.map_partitions(create_gamestate_delta)\n",
        "\n",
        "    # Additional Feature Engineering Steps\n",
        "    def additional_feature_engineering(df):\n",
        "        df['pitch_speed_diff'] = df['release_speed'] - df['effective_speed']\n",
        "        df['pitch_location_diff'] = df['plate_x'] - df['plate_z']\n",
        "        df['pitch_count'] = df.groupby(['game_pk', 'at_bat_number']).cumcount() + 1\n",
        "        df['is_strike'] = df['events'].apply(lambda x: 1 if x in ['strikeout', 'strike'] else 0)\n",
        "        return df\n",
        "\n",
        "    pitch_df = pitch_df.map_partitions(additional_feature_engineering)\n",
        "\n",
        "    # Data Visualization (Dask doesn't directly support visualization, so we convert to pandas for plotting)\n",
        "    def visualize_data(df, column, title):\n",
        "        df = df.compute()\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(df[column], bins=30, kde=True)\n",
        "        plt.title(title)\n",
        "        plt.xlabel(column)\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.show()\n",
        "\n",
        "    # Visualize gamestate_delta\n",
        "    visualize_data(pitch_df, 'gamestate_delta', 'Distribution of Gamestate Delta')\n",
        "\n",
        "    # Visualize pitch_speed_diff\n",
        "    visualize_data(pitch_df, 'pitch_speed_diff', 'Distribution of Pitch Speed Difference')\n",
        "\n",
        "    # Visualize pitch_location_diff\n",
        "    visualize_data(pitch_df, 'pitch_location_diff', 'Distribution of Pitch Location Difference')\n",
        "\n",
        "    # Define the transformer model (similar to BERT architecture)\n",
        "    config = BertConfig(\n",
        "        vocab_size=325,  # Number of unique gamestate deltas\n",
        "        hidden_size=512,\n",
        "        num_hidden_layers=8,\n",
        "        num_attention_heads=8,\n",
        "        intermediate_size=2048\n",
        "    )\n",
        "    model = BertModel(config)\n",
        "\n",
        "    # Tokenizer for the transformer model\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Training the model using MGM and Contrastive Learning\n",
        "    def train_transformer_model(model, tokenizer, df, epochs=3, model_path='transformer_model.pth'):\n",
        "        optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "        if os.path.exists(model_path):\n",
        "            model.load_state_dict(torch.load(model_path))\n",
        "            logger.info(f\"Loaded model from {model_path}\")\n",
        "        else:\n",
        "            for epoch in range(epochs):\n",
        "                model.train()\n",
        "                for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "                    inputs = tokenizer(str(row['gamestate_delta']), return_tensors='pt')\n",
        "                    outputs = model(**inputs)\n",
        "                    loss = outputs.loss\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            logger.info(f\"Saved model to {model_path}\")\n",
        "\n",
        "    # Convert Dask DataFrame to Pandas DataFrame for training the transformer model\n",
        "    pitch_df_pd = pitch_df.compute()\n",
        "    train_transformer_model(model, tokenizer, pitch_df_pd)\n",
        "\n",
        "    # Function to prepare data for a specific pitcher and batter\n",
        "    def prepare_matchup_data(pitch_df, season_df, pitcher, batter, pitcher_year, batter_year):\n",
        "        pitcher_stats = season_df[(season_df['player'] == pitcher) & (season_df['year'] == pitcher_year)].compute()\n",
        "        batter_stats = season_df[(season_df['player'] == batter) & (season_df['year'] == batter_year)].compute()\n",
        "\n",
        "        if pitcher_stats.empty or batter_stats.empty:\n",
        "            raise ValueError(\"Pitcher or Batter not found for the given year\")\n",
        "\n",
        "        matchup_df = pitch_df[(pitch_df['pitcher'] == pitcher_stats['player_id'].values[0]) &\n",
        "                              (pitch_df['batter'] == batter_stats['player_id'].values[0])].compute()\n",
        "\n",
        "        return matchup_df\n",
        "\n",
        "    # Function to predict the outcome of a matchup\n",
        "    def predict_matchup(pitch_df, season_df, pitcher, batter, pitcher_year, batter_year, model):\n",
        "        matchup_df = prepare_matchup_data(pitch_df, season_df, pitcher, batter, pitcher_year, batter_year)\n",
        "\n",
        "        if matchup_df.empty:\n",
        "            logger.info(\"No data available for this matchup.\")\n",
        "            return\n",
        "\n",
        "        X_matchup = matchup_df.drop(columns=['events'])  # Replace 'events' with the actual target column\n",
        "        y_matchup = matchup_df['events']  # Replace 'events' with the actual target column\n",
        "\n",
        "        y_pred = model.predict(X_matchup)\n",
        "        accuracy = accuracy_score(y_matchup, y_pred)\n",
        "        conf_matrix = confusion_matrix(y_matchup, y_pred)\n",
        "        class_report = classification_report(y_matchup, y_pred)\n",
        "\n",
        "        logger.info(f'Matchup Prediction Accuracy: {accuracy * 100:.2f}%')\n",
        "        logger.info(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "        logger.info(f'Classification Report:\\n{class_report}')\n",
        "\n",
        "    # Split the data into training and test sets\n",
        "    X = pitch_df_pd.drop(columns=['events'])  # Replace 'events' with the actual target column\n",
        "    y = pitch_df_pd['events']  # Replace 'events' with the actual target column\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a RandomForest model to predict game outcomes\n",
        "    logger.info(\"Training RandomForest model...\")\n",
        "\n",
        "    # Define the objective function for Optuna\n",
        "    def objective(trial):\n",
        "        n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
        "        max_depth = trial.suggest_int('max_depth', 2, 32)\n",
        "        min_samples_split = trial.suggest_int('min_samples_split', 2, 16)\n",
        "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 16)\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            min_samples_split=min_samples_split,\n",
        "            min_samples_leaf=min_samples_leaf,\n",
        "            random_state=42\n",
        "        )\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, preds)\n",
        "        return accuracy\n",
        "\n",
        "    # Run the Optuna study\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=50)\n",
        "\n",
        "    # Get the best hyperparameters and train the final model\n",
        "    best_params = study.best_params\n",
        "    logger.info(f\"Best hyperparameters: {best_params}\")\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators=best_params['n_estimators'],\n",
        "        max_depth=best_params['max_depth'],\n",
        "        min_samples_split=best_params['min_samples_split'],\n",
        "        min_samples_leaf=best_params['min_samples_leaf'],\n",
        "        random_state=42\n",
        "    )\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Cross-validation\n",
        "    logger.info(\"Performing cross-validation...\")\n",
        "    cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5)\n",
        "    logger.info(f'Cross-validation scores: {cv_scores}')\n",
        "    logger.info(f'Mean cross-validation score: {cv_scores.mean()}')\n",
        "\n",
        "    # Evaluate the model\n",
        "    logger.info(\"Evaluating RandomForest model...\")\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    class_report = classification_report(y_test, y_pred)\n",
        "    logger.info(f'Random Forest Model Accuracy: {accuracy * 100:.2f}%')\n",
        "    logger.info(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "    logger.info(f'Classification Report:\\n{class_report}')\n",
        "\n",
        "    # Save the RandomForest model\n",
        "    rf_model_path = 'random_forest_model.pkl'\n",
        "    joblib.dump(rf_model, rf_model_path)\n",
        "    logger.info(f\"Saved RandomForest model to {rf_model_path}\")\n",
        "\n",
        "    # Analysis of feature importance\n",
        "    importances = rf_model.feature_importances_\n",
        "    logger.info(f'Feature Importances: {importances}')\n",
        "\n",
        "    # Example usage for predicting matchup outcomes\n",
        "    pitcher = 'Gerrit Cole'  # Replace with user input\n",
        "    batter = 'Mike Trout'    # Replace with user input\n",
        "    pitcher_year = 2021      # Replace with user input\n",
        "    batter_year = 2021       # Replace with user input\n",
        "\n",
        "    try:\n",
        "        predict_matchup(pitch_df, season_df, pitcher, batter, pitcher_year, batter_year, rf_model)\n",
        "    except ValueError as e:\n",
        "        logger.error(e)\n",
        "\n",
        "    # Closing the database connection\n",
        "    conn.close()\n",
        "\n",
        "# Unit Tests\n",
        "class TestMLBDataProcessing(unittest.TestCase):\n",
        "    def test_create_gamestate_delta(self):\n",
        "        test_df = pd.DataFrame({'balls': [1, 2, 3], 'strikes': [0, 1, 2]})\n",
        "        result_df = create_gamestate_delta(test_df)\n",
        "        expected_df = pd.DataFrame({'balls': [1, 2, 3], 'strikes': [0, 1, 2], 'gamestate_delta': [1, 1, 1]})\n",
        "        pd.testing.assert_frame_equal(result_df, expected_df)\n",
        "\n",
        "    def test_additional_feature_engineering(self):\n",
        "        test_df = pd.DataFrame({'release_speed': [90, 95, 100], 'effective_speed': [85, 90, 95], 'plate_x': [0.5, 0.4, 0.3], 'plate_z': [0.2, 0.1, 0.0]})\n",
        "        result_df = additional_feature_engineering(test_df)\n",
        "        expected_df = pd.DataFrame({\n",
        "            'release_speed': [90, 95, 100],\n",
        "            'effective_speed': [85, 90, 95],\n",
        "            'plate_x': [0.5, 0.4, 0.3],\n",
        "            'plate_z': [0.2, 0.1, 0.0],\n",
        "            'pitch_speed_diff': [5, 5, 5],\n",
        "            'pitch_location_diff': [0.3, 0.3, 0.3],\n",
        "            'pitch_count': [1, 1, 1],\n",
        "            'is_strike': [0, 0, 0]\n",
        "        })\n",
        "        pd.testing.assert_frame_equal(result_df, expected_df)\n",
        "\n",
        "    def test_data_loading(self):\n",
        "        conn = sqlite3.connect('mlb_data.db')\n",
        "        self.assertTrue('pitch_data' in conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall())\n",
        "\n",
        "    def test_model_training(self):\n",
        "        rf_model_path = 'random_forest_model.pkl'\n",
        "        rf_model = joblib.load(rf_model_path)\n",
        "        X_sample = X_test.sample(n=10, random_state=42)\n",
        "        y_sample = y_test.loc[X_sample.index]\n",
        "        y_pred = rf_model.predict(X_sample)\n",
        "        accuracy = accuracy_score(y_sample, y_pred)\n",
        "        self.assertGreater(accuracy, 0.5, \"Model accuracy should be greater than 50%\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    unittest.main()"
      ],
      "metadata": {
        "id": "sGoHBZXxRi6n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01201453-c6a3-475c-9ad9-8c3a3d907ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
            "Perhaps you already have a cluster running?\n",
            "Hosting the HTTP server on port 38889 instead\n",
            "  warnings.warn(\n",
            "INFO:distributed.scheduler:State start\n",
            "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:40145\n",
            "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:38889/status\n",
            "WARNING:distributed.nanny.memory:Ignoring provided memory limit 16GB due to system memory limit of 12.67 GiB\n",
            "WARNING:distributed.nanny.memory:Ignoring provided memory limit 16GB due to system memory limit of 12.67 GiB\n",
            "WARNING:distributed.nanny.memory:Ignoring provided memory limit 16GB due to system memory limit of 12.67 GiB\n",
            "WARNING:distributed.nanny.memory:Ignoring provided memory limit 16GB due to system memory limit of 12.67 GiB\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:46153'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:44191'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:32877'\n",
            "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:39253'\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:46727', name: 0, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:46727\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:37942\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:41063', name: 3, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:41063\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:37970\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:40995', name: 2, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:40995\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:37978\n",
            "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:37817', name: 1, status: init, memory: 0, processing: 0>\n",
            "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:37817\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:37954\n",
            "INFO:distributed.scheduler:Receive client connection: Client-5a75e4d7-386e-11ef-8181-0242ac1c000c\n",
            "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:37988\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is a large query, it may take a moment to complete\n",
            "Skipping offseason dates\n",
            "Skipping offseason dates\n",
            "Skipping offseason dates\n",
            "Skipping offseason dates\n",
            "Skipping offseason dates\n",
            "Skipping offseason dates\n",
            "Skipping offseason dates\n",
            "Skipping offseason dates\n",
            "Skipping offseason dates\n",
            "Skipping offseason dates\n",
            "Skipping offseason dates\n",
            "Skipping offseason dates\n",
            "Skipping offseason dates\n",
            "Skipping offseason dates\n",
            "Skipping offseason dates\n",
            "Skipping offseason dates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 267/3528 [09:18<2:29:48,  2.76s/it]"
          ]
        }
      ]
    }
  ]
}